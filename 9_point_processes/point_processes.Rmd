---
title: "Point Processes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>

## Question 1 - A bit of statistics with Poisson processes

We are observing two independent spike trains on the same interval $[0, T]$ and we want to test if their firing rate is the same. We will assume here that we have two independent Poisson processes $N^a$ and $N^b$ with rate $\lambda_a(.)$ and $\lambda_b(.)$, not necessarily constant.

**A** -- What is the distribution of $N^a_T$ and $N^b_T$, respectively the number of points of $N^a$ and $N^b$ in $[0,T]$? Are these variables independent?

**B** -- Compute $\mathbb{P}(N^a_T=k \cup N^b_T=l)$ for all $k$ and $l$.

**C** -- Let $N_T= N^a_T+N^b_T$. Compute $\mathbb{P}(N_t=n)$ for all $n$. Can you give the name and the parameters of this distribution?

**D** -- Compute $\mathbb{P}(N^a_t=k|N_t=n)$. *You can first see that $k$ has to be less than $n$ and that in this case $N^b_T = n-k$*. Can you give the name and the parameters of this conditional distribution?

**E** -- Under the null hypothesis that $\lambda_a(.) = \lambda_b(.)$, realize that the previous distribution is known and derive from it a test of $H_0:\lambda_a(.)=\lambda_b(.)$ versus $H_1:\lambda_a(.)\ge\lambda_b(.)$

**F** -- Apply it on the data of the STAR package for two different neurons. NB: *If $N^1, ..., N^n$ are IID Poisson processes with intensity $\lambda(.)$ then $N = N^1\cup ...\cup N^n$ is a Poisson process with rate $n\lambda(.)$. You can use that to glue all the trials together.*

### step A

Given that the two independent Poisson processes $N^a$ and $N^b$ don't have necessarily constant rates $\lambda_a(.)$ and $\lambda_b(.)$, we can consider them inhomogeneous Poisson process over an interval $[0,T]$. As such, given the random variables $N^a_T$ and $N^b_T$ the number of points of $N^a$ and $N^b$ that happened in the interval $[0,T]$, we have:

\begin{align}
N^a_T&\sim\mathbb{P}\big(\int_{[0,T]}\lambda_a(x)dx\big)\\
N^b_T&\sim\mathbb{P}\big(\int_{[0,T]}\lambda_b(x)dx\big)\\
\text{Where}&,\\
\lambda_a&,\text{ the intensity of the process $a$}\\
\lambda_b&,\text{ the intensity of the process $b$}
\end{align}

Based on [wikipedia](https://en.wikipedia.org/wiki/Poisson_point_process)'s formulas, we can state the following:

\begin{align}
\Lambda_a(0, T)&=\int_{[0,T]}\lambda_a(x)dx\\
\Lambda_b(0, T)&=\int_{[0,T]}\lambda_b(x)dx\\
P(N^a_T=k)&=\frac{\big[\Lambda_a(0, T)\big]^k}{k!}.e^{-\Lambda_a(0, T)}\\
P(N^a_T=l)&=\frac{\big[\Lambda_b(0, T)\big]^l}{l!}.e^{-\Lambda_b(0, T)}
\end{align}

Given that the two Poisson processes $N^a$ and $N^b$ are independent, we can consider the random variables $N^a_T$ and $N^b_T$ to be independent as well.

### step B

Based on our independence hypothesis stated in step A, we can say that, for two random variable $X$ and $Y$ that $P(X\cup Y)=P(X).P(Y)$. As such:

\begin{align}
\mathbb{P}(N^a_T=k \cup N^b_T=l)&=\mathbb{P}(N^a_T=k) * \mathbb{P}(N^b_T=l)\\
&= \frac{\big[\Lambda_a(0, T)\big]^k}{k!}.e^{-\Lambda_a(0, T)} * \frac{\big[\Lambda_b(0, T)\big]^l}{l!}.e^{-\Lambda_b(0, T)}\\
&= \frac{\big[\Lambda_a(0, T)\big]^k\big[\Lambda_b(0, T)\big]^l}{k!l!}.e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}\\
&= \frac{\big[\Lambda_a(0, T)\big]^k\big[\Lambda_b(0, T)\big]^l}{k!l!}.e^{-(\int_{[0,T]}\lambda_a(x)+\lambda_b(x)dx)}
\end{align}

### step C

We previously found:

\begin{align}
\mathbb{P}(N^a_T=k \cup N^b_T=l)&= \frac{\big[\Lambda_a(0, T)\big]^k\big[\Lambda_b(0, T)\big]^l}{k!l!}.e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}\\
\end{align}

As such, we can state:

\begin{align}
\mathbb{P}(N^a_T+N^b_T=n)&=\mathbb{P}(N^a_T=k, N^b_T=n-k)\quad\forall n,k\in\mathbb{N} \text{ s.t. } n-k>=0\\
&=\binom{n}{k}\mathbb{P}(N^a_T=k \cup N^b_T=n-k)\\
&=\binom{n}{k}\frac{\big[\Lambda_a(0, T)\big]^k\big[\Lambda_b(0, T)\big]^{n-k}}{k!(n-k)!}.e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}\\
&=e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}.\binom{n}{k}\frac{\big[\Lambda_a(0, T)\big]^k\big[\Lambda_b(0, T)\big]^{n-k}}{k!(n-k)!}\\
\end{align}

Relying on the binomial formula, we can then state:

\begin{align}
\mathbb{P}(N^a_T+N^b_T=n)&=e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}.\frac{\big[\Lambda_a(0, T)+\Lambda_b(0, T)\big]^{n}}{n!}\\
\end{align}

As such, we find:

The sum of the random variables $N^a_T$ and $N^b_T$ also follows a Poisson distribution with parameter/rate $\Lambda_a(0, T)+\Lambda_b(0, T)$.

\begin{align}
N^a_T+N^b_T&\sim\mathbb{P}\big(\int_{[0,T]}\lambda_a(x)+\lambda_b(x)dx\big)\\
\end{align}

This can be verified with the Theorem 18.2 "Superposition of independent Poisson processes" (Source: [MAT135B: Stochastic Processes, complete lecture notes, Spring 2011, page 200 ](https://www.math.ucdavis.edu/~gravner/MAT135B/materials/)), reproduced here:

- *Assume that $N_1(t)$ and $N_2(t)$ are independent Poisson processes with rates $\lambda_1$ and $\lambda_2$. Combine them into a single process by taking the union of both sets of events or, equivalently, $N(t) =
N_1(t) + N_2(t)$. This is a Poisson process with rate $\lambda_1 + \lambda_2$.*

### step D

We know that:

\begin{align}
P(X,Y)&=P(X).P(Y)\quad\text{when $X$ and $Y$ are independent}\\
P(X|Y)&=\frac{P(X,Y)}{P(Y)}
\end{align}

As such:

\begin{align}
\mathbb{P}(N^a_T=k|N_T=n)&=\frac{\mathbb{P}(N^a_T=k,N_T=n)}{\mathbb{P}(N_T=n)}\\
&=\frac{\mathbb{P}(N^a_T=k,N^a_T + N^b_T=n)}{\mathbb{P}(N^a_T + N^b_T=n)}\\
&=\frac{\mathbb{P}(N^a_T=k,N^b_T=n-k)}{e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}.\frac{\big[\Lambda_a(0, T)+\Lambda_b(0, T)\big]^{n}}{n!}}\\
&=\frac{\mathbb{P}(N^a_T=k).\mathbb{P}(N^b_T=n-k)}{e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}.\frac{\big[\Lambda_a(0, T)+\Lambda_b(0, T)\big]^{n}}{n!}}\\
&=\frac{\frac{\big[\Lambda_a(0, T)\big]^k}{k!}.e^{-\Lambda_a(0, T)}.\frac{\big[\Lambda_b(0, T)\big]^{n-k}}{(n-k)!}.e^{-\Lambda_b(0, T)}}{e^{-(\Lambda_a(0, T)+\Lambda_b(0, T))}.\frac{\big[\Lambda_a(0, T)+\Lambda_b(0, T)\big]^{n}}{n!}}\\
&=\frac{\frac{\big[\Lambda_a(0, T)\big]^k}{k!}.\frac{\big[\Lambda_b(0, T)\big]^{n-k}}{(n-k)!}}{\frac{\big[\Lambda_a(0, T)+\Lambda_b(0, T)\big]^{n}}{n!}}\\
&=\frac{n!\big[\Lambda_a(0, T)\big]^k.\big[\Lambda_b(0, T)\big]^{n-k}}{k!(n-k)!\big[\Lambda_a(0, T)+\Lambda_b(0, T)\big]^{n}}\\
&=\binom{n}{k}\frac{\big[\Lambda_a(0, T)\big]^k.\big[\Lambda_b(0, T)\big]^{n-k}}{\big[\Lambda_a(0, T)+\Lambda_b(0, T)\big]^{n}}\\
&=\binom{n}{k}\frac{\big[\Lambda_a(0, T)\big]^k.\big[\Lambda_b(0, T)\big]^{n-k}}{\underset{k'=0}{\overset{n}{\sum}}\binom{n}{k'}\Lambda_a(0, T)^{k'}\Lambda_b(0, T)^{n-k'}}\\
\end{align}

As such, we find:



### step E



### step F



<hr>

## Question 2 - Simulation

**A** -- Simulate by thinning a Poisson process with intensity $t\rightarrow h(t) = (1-t^2)$ when $t\in[0,1]$ and $0$ elsewhere.

**B** -- How many points should produce such a process in average? Verify it on you computer.

**C** -- Simulate parents $T_p$ according to a Poisson process of rate $M$. Then for each parent simulate their children that have appeared after them according to a Poisson process with intensity $h(t-T_p)$. 

**D** -- Interpret this process as a particular case of Multivariate Hawkes process with 2 processes, for which you will give spontaneous parameters and intercation functions. Find another way to simulate this by success thinning. *NB: These algorithms are complex, try first to write it down on a sheet of paper before implementing them*

### step A


Overall, the intensity function decreases over the space $t\in[0, 1]$ as such:

```{r, out.width="100%"}

func <- function(t){1-t^2}
c = seq(1,100,1)/100
plot(c, func(c), type="l")

```

Intuitively, we would expect the poisson process to not always output a single point as the intensity is a decreasing function of time. 

So, given the intensity $h(t)=(1-t^2)$, we set $M=1$ as $\forall t\in [0, 1],\,h(t)<=1$. We produce the following thinning simulator:

```{r}

poisson_process_simulation_thinning <- function(){
  ### Simulate by thinning a Poisson process with intensity t->h(t)=(1−t2) 
  ### when t∈[0,1] and 0 elsewhere.
  M = 1
  Tmax = 1
  func <- function(t){rbinom(1,1,1-t^2)}
  simulation = cumsum(rexp(100, M))
  simulation = matrix(simulation[simulation<=Tmax])
  if (length(simulation)==0) {
    return(NA)
  }
  ret = c()
  for (i in 1:length(simulation)){
    if (func(simulation[i])==1){
      ret = cbind(ret, simulation[i])
    }
  }
  if (length(ret)==0){
    return(NA)
  } else {
    ret[,!is.na(ret)]
  }
}

```

Then we compute 100 simulations of this thinning process as example:

```{r}

for (i in 1:100){
  cat("Simulation #", i, ": ", poisson_process_simulation_thinning(), "\n")
}

```

### step B

On average on $t\in[0,1]$, we have: 

\begin{align}
N_[0,1]&\sim\mathbb{P}(\int_0^1(1-t^2)dt)\\
\mathbb{E}[N_{[0,1]}]&=\int_0^1(1-t^2)dt\\
&=\big[t-\frac{t^3}{3}\big]^1_0\\
&=1-\frac{1}{2}\\
&=\frac{2}{3}
\end{align}

The average amount of created points should be $\frac{2}{3}$ over $t\in[0, 1]$. We can confirm this by going back to our simulation function and count the average number of generated points over the total number of simulations.


```{r}

count_nb_point_generated <- function(){
  ### Counts the number of generated points during 1000 poisson process simulation with thinning
  counter = 0
  for (i in 1:1000){
    simulation = poisson_process_simulation_thinning()
    if (length(simulation)==0 || !is.na(simulation)){counter = counter + length(simulation)}
  }
  return(list("nb"=counter, "rate"=counter/1000))
}

example = count_nb_point_generated()
cat("Example -- ", example$nb, " points were generated over 1000 simulations, i.e. a rate of ", example$rate, ".")
```

We plot out the empirical distribution of the simulated rate after 1000 batches of 1000 simulations with, in red, the previously computed expected average number of generated points.

```{r, out.width="100%"}
distribution_of_counter = apply(matrix(c(1:1000)), 1, function(x){count_nb_point_generated()$rate})
cat("Mean rate obtained after 1000 batches of 1000 simulations", mean(distribution_of_counter))
plot(density(distribution_of_counter))
abline(v=2/3, col="red")
```

Our empirical observation seems to match our mathematical computation.

### step C



### step D

